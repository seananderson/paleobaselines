\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\textheight 20.5cm
\setlength\parskip{0.11in}
\setlength\parindent{0in}
\let\proglang=\textsf
\let\pkg=\textbf
\let\fn=\texttt

\usepackage{fontspec}
\defaultfontfeatures{Ligatures=TeX}
\setsansfont{Calibri}
\setmonofont{Inconsolata}
\setmainfont{Linux Libertine}

\usepackage[nofiglist,notablist,nomarkers,figuresfirst]{endfloat}
\renewcommand{\theposttable}{S\arabic{posttbl}}
\renewcommand{\thepostfigure}{S\arabic{postfig}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\figurename}{Figure}
\renewcommand{\tablename}{Table}

\begin{document}

<<set-knitr-options, cache=FALSE, echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.align='center', fig.pos="htpb", cache=TRUE, echo=TRUE,
message=FALSE, autodep=TRUE, fig.path='../figs/', fig.width=6, par=TRUE)
opts_chunk$set(warning=FALSE, message=FALSE, tidy=FALSE, refresh=TRUE,
  cache.path="../cache/", cache.lazy=FALSE)
opts_chunk$set(dev = 'pdf')
opts_knit$set(out.format = "latex")
thm <- knit_theme$get("solarized.css")
knit_theme$set(thm)
@

\title{Supplementary Materials for Paleontological Baselines for Evaluating Contemporary Extinction Risk in the Coastal Oceans}
\author{Seth Finnegan\textsuperscript{$\ast$} \and Sean C. Anderson \and Paul G. Harnik \and Carl Simpson \and Derek P. Tittensor \and Jarrett E. Byrnes \and Zoe V. Finkel \and David R. Lindberg \and Lee Hsiang Liow \and Rowan Lockwood \and Heike K. Lotze \and Craig M. McClain \and Jenny L. McGuire \and Aaron O'Dea \and John M. Pandolfi}
\date{}
\maketitle

\textsuperscript{$\ast$}Corresponding author. E-mail sethf@berkeley.edu

This PDF file includes:\\
Figs.\ S1 to SXX\\
References

\clearpage

\tableofcontents

\clearpage

This document is written with the \pkg{knitr} package \cite{xie2013a,xie2013b,xie2013c} for the \proglang{R} statistical environment \cite{r2013}. It can be re-created by running \fn{knitr::knit2pdf()} on the \texttt{.Rnw} file. A number of functions that are used in this analysis have been compiled into an \proglang{R} package \pkg{paleorisk}.

You can install the package by downloading the \texttt{paleorisk} folder and running:

<<install-paleorisk, eval=FALSE, echo=TRUE>>=
install.packages("path/to/paleorisk/", repos = NULL, type = "source")
@

We will start by loading this package:

<<load-this-package, cache=FALSE>>=
library("paleorisk")
@

Then we will load other packages that are needed for our analysis:

<<load-other-packages, cache=FALSE>>=
library("plyr")
library("dplyr")
library("reshape2")
library("ggplot2")
library("DMwR")
library("gbm")
library("maps")
library("mapproj")
library("maptools")
library("RColorBrewer")
gpclibPermit()
@

<<preamble-work, cache=FALSE, echo=FALSE, results=FALSE>>=
theme_set(theme_bw())
rversion <- capture.output(sessionInfo())[[1]]
x <- capture.output(sessionInfo())
other_line <- grep("other attached", x)
pkgs <- x[(other_line+1):(other_line + 5)]
@

We used \Sexpr{rversion} and package versions:

<<versions, echo=FALSE, results=TRUE, cache=FALSE>>=
pkgs
@

\section{Choice of taxa}

\section{Data sources}

\section{Standardizing the taxonomic data}

\section{Interpolation of OBIS occurrence data}

Describe process and link to the code that runs this.

\section{Choice of prediction variables}

\section{Standardizing the predictors}

We will start by standardizing a variety of habitat-relative characteristics within each geologic stage. This is an important step to make some habitat characteristics (richness, occupancy, occurrences) comparable across periods, an in particular, between the paleontological data and the contemporary data. Given the substantially different sampling methods within the paleontological and contemporary records, the absolute values of richness, occupancy, and occurrences are not comparable. We therefore standardize them to values between 0 and 1 within each interval. The other variables (e.g.\ latitude variables and great circle distance) are still comparable between the paleontological and contemporary records --- a degree latitude is still a degree latitude and a kilometre is still a kilometre.

The following script creates the \proglang{R} data object \texttt{stand-predictors-cen-obis.rds}:

<<standardize-predictors>>=
data <- readRDS("../data/modern-paleodb-ranges.rds")
mean.lats <- readRDS("../data/genus-mean-occurence-lats.rds")
data <- merge(data, mean.lats)
stage_names <- unique(data$Interval_Name)
stcen <- ldply(stage_names, function(x)
  standardize_data(data, interval_name = x))
saveRDS(stcen, file = "../data/stand-predictors-cen-obis.rds")

# Calibrate based on Plio-Pleistocene?
stcen <- calibrate_fossil(stcen)
@

We'll set some global variables. These should be integrated elsewhere.

<<legacy-globals>>=
Min_Occupancy <- 1
# choose the minimum number of PaleoDB localities for inclusion
Min_PBDB_Loc <- 1
# choose the minimum number of OBIS localities for inclusion
Min_OBIS_Loc <- 1
# choose the minimum number of Spalding provinces occupied for inclusion (applies only to modern genera)
Min_Prov <- 1
# set minimum great circle distance for inclusion (single occurrence PBDB genera assigned great.circle = 200)
Min_gcd <- 1
@

\section{Model building, testing, and calibration}

We will load the standardized data and subset it for the Neogene period. The contemporary data are also contained in this data frame with \texttt{stage\_top = 0}. Therefore, we will also exclude the contemporary dataset from what we call \texttt{stcen}.

<<load-data>>=
stcen <- readRDS("../data/stand-predictors-cen-obis.rds")
stcen <- subset(stcen, class != "Testudines")
stcen <- gdata::drop.levels(subset(stcen, stcen$stage_top < 22 &
    stcen$stage_top != 0))
@

\fn{validate\_gbm()} in the \pkg{paleorisk} package is our main validation and calibration function. This function deals with (possibly) splitting the data into training and testing portions, builds a generalized boosted regression model (GBM \cite{gbm2013}) on the training portion, and then returns predictions and classification statistics on the testing set.

We will start by testing some GBM models. To start with, we will build the model 25 times, each time training the model on a random 50\% of the data and testing it on the other 50\%. Then we will record the observed mean extinction probability for each predicted bin value within each taxonomic class. For example, we can group all predicted extinction values for bivalves into bins from 0 to 1 in 0.05 increments. Then we can calculate the mean observed extinction probability for each of those bins. Because we're building and testing the model on independent datasets, this is a good test of the out-of-sample predictive ability of our modelling approach.

<<test-gbm1>>=
val_test <- rdply(25, validate_gbm(stcen, shrinkage = 0.1,
  interaction.depth = 1, n.trees = 300, use_weights = TRUE)$bin_validation)
@

Next, we will compute some summary statistics for the observed extinction probability at each bin across our replicates. We will use the \fn{summarize\_val\_test()} from our \pkg{paleorisk} package. This computes means and 95\% confidence intervals (based on likelihood profiles) for each validation bin.

<<test-gbm1-summary>>=
val_test_summarized <- ddply(val_test, c("class", "gbm_pred_binned"),
  summarize_val_test)
@

We can plot these data to test for any non-linear biases in our predictions. Translucent black dots show mean observed extinction probabilities for individual replicates. Blue dots show mean observed extinction probabilities across replicates and line segments show 95\% confidence intervals. The area of the dots corresponds to the median number of extinction/survival observations in that bin across replicates.

<<test-gbm1-bias-plot, fig.width=7.5, fig.height=7>>=
ggplot(val_test_summarized,
  aes(gbm_pred_binned, mean_observed)) +
  geom_point(data = val_test, aes(gbm_pred_binned, obs_ext_prob),
    alpha = 0.1) +
  geom_linerange(aes(ymin = l, ymax = u), colour = "#4E9DF0") +
  geom_point(aes(size = median_sample_n), colour = "#4E9DF0") +
  facet_wrap(~class) +
  scale_size(trans = "log10") + coord_fixed(ratio = 1) +
  ylim(0, 1) + xlim(0, 1) +
  xlab("Out-of-sample GBM-predicted extinction probability") +
  ylab("Observed extinction probability") +
  labs(size = "Median sample #")
@

Next, we can test the effect of the shrinkage, interaction depth, and number of trees arguments to \texttt{gbm()}. We will vary each argument one at a time.

<<alternative-gbms>>=
val_test_base <- rdply(20, validate_gbm(stcen, shrinkage = 0.1,
    interaction.depth = 1, n.trees = 300, use_weights = TRUE)$bin_validation)
val_test_shr <- rdply(20, validate_gbm(stcen, shrinkage = 0.01,
    interaction.depth = 1, n.trees = 300, use_weights = TRUE)$bin_validation)
val_test_int <- rdply(20, validate_gbm(stcen, shrinkage = 0.1,
    interaction.depth = 4, n.trees = 300, use_weights = TRUE)$bin_validation)
val_test_ntrees <- rdply(20, validate_gbm(stcen, shrinkage = 0.1,
    interaction.depth = 1, n.trees = 1000, use_weights = TRUE)$bin_validation)
@

<<summarize-alt-gbms>>=
val_test_base_summ <- ddply(val_test_base, c("class", "gbm_pred_binned"),
  summarize_val_test)
val_test_shr_summ <- ddply(val_test_shr, c("class", "gbm_pred_binned"),
  summarize_val_test)
val_test_int_summ <- ddply(val_test_int, c("class", "gbm_pred_binned"),
  summarize_val_test)
val_test_ntrees_summ <- ddply(val_test_ntrees, c("class", "gbm_pred_binned"),
  summarize_val_test)
@

<<rename-alt-gbms>>=
val_test_base_summ$version <- "Base"
val_test_shr_summ$version <- "Smaller shrinkage"
val_test_int_summ$version <- "Larger interaction"
val_test_ntrees_summ$version <- "More trees"
val_test_alt <- rbind(val_test_base_summ, val_test_shr_summ,
  val_test_int_summ, val_test_ntrees_summ)
@

We will plot out the observed vs.\ predicted for these 4 versions of the model. We see that a larger interaction value results in near universally poorer predictive performance. Generating more trees results in similar performance at considerably increased computational time. A smaller shrinkage value (slower learning between trees) substantially reduces the range of predicted extinction probabilities, but frequently results in more biased predictions --- even in some bins with large sample sizes. We therefore continue from here on with the model we refer to as the ``base'' model above.

<<plot-alt-gbms, fig.height=8, fig.width=13, out.width="7in">>=
ggplot(val_test_alt,
  aes(gbm_pred_binned, mean_observed)) +
  geom_point(aes(size = median_sample_n)) +
  geom_linerange(aes(ymin = l, ymax = u)) +
  facet_grid(version~class) +
  scale_size(trans = "log10") + coord_fixed(ratio = 1) +
  ylim(0, 1) + xlim(0, 1) +
  xlab("Out-of-sample GBM-predicted extinction probability") +
  ylab("Observed extinction probability") +
  labs(size = "Median sample #")
@

We note that Malacostraca is poorly predicted in our out-of-sample validation tests. We therefore choose here to not include Malacostraca from here on in. If we can't reliably predict Malacostraca extinctions in the paleontological data then we can't justify attempting to predict contemporary patterns of extinction.

<<drop-malacostraca>>=
stcen2 <- gdata::drop.levels(subset(stcen, class != "Malacostraca"))
@

We will run many more cross-validation iterations of our model to get precise estimates of non-linear bias so that we can use the observed mean extinction probabilities as calibration factors:

<<final-gbm-validation, eval=TRUE>>=
val_test <- rdply(50, validate_gbm(stcen2, shrinkage = 0.1,
  interaction.depth = 1, n.trees = 300,
  use_weights = TRUE)$bin_validation)
saveRDS(val_test, file = "../data/val_test.rds")
@

<<final-gbm-summarize, eval=TRUE>>=
val_test_summarized <- ddply(val_test, c("class", "gbm_pred_binned"),
  summarize_val_test)
@

And we will plot these validation values again:

<<plot-gbm-no-mal, fig.width=8, fig.height=5>>=
ggplot(val_test_summarized,
  aes(gbm_pred_binned, mean_observed)) +
  geom_point(data = val_test, aes(gbm_pred_binned, obs_ext_prob),
    alpha = 0.1) + facet_wrap(~class) +
  geom_abline(intercept = 0, slope = 1) +
  geom_linerange(aes(ymin = l, ymax = u), colour = "#4E9DF0") +
  geom_point(aes(size = median_sample_n), colour = "#4E9DF0") +
  scale_size(trans = "log10") + coord_fixed(ratio = 1) +
  ylim(0, 1) + xlim(0, 1) +
  xlab("Out-of-sample GBM-predicted extinction probability") +
  ylab("Observed extinction probability") +
  labs(size = "Median sample #")
@

We can use these observed extinction probabilities (in \texttt{val\_test\_summarized}) as calibration for our GBM-predicted extinction probability in the contemporary dataset. To do this, we take the predicted extinction probability from our GBM model applied to the contemporary dataset, we fit that value into our binning scheme, and we match it up with the empirically-observed mean extinction probability for that bin. This removes the observed non-linear bias in our model \cite{thorson2012}. Put another way, we find our predicted value on the x-axis of the last plot and draw a vertical line up until we meet a blue dot. We then use the value of the blue dot as our calibrated extinction probability. We will come back to that shortly when we apply our model to the contemporary dataset.

\section{Testing predictive performance across intervals}

Another way to test the predictive power of our models is to build the model on one geologic stage and predict on another. To keep this simple, we won't use our calibration factors.

<<cross-intervals-gbms>>=
stage_df <- expand.grid(stage_train = unique(stcen2$stage),
  stage_test = unique(stcen2$stage))
cross_pred <- mdply(stage_df, fit_interval_gbm, dat = stcen2)
@

We can compare the predictions made from other stages with the predictions from the same stage. This code produces Figure~\ref{fig:plot-cross-interval-gbms}.

<<plot-cross-interval-gbms, fig.cap="Cross-stage prediction tests. The x-axis shows the probability of extinction as predicted by a model built on data from the same stage. The y-axis shows the probability of extinction as predicted by a model built on data from a different stage. The colours show different taxonomic classes. Note the log-distributed x- and y-axes. Since there is some stochasticity involved in the \\texttt{gbm()} fitting routine, there is some variability in predictions within the same stage (the diagonal panels).", fig.width=9, fig.height=7, cache=TRUE>>=
# Re-order stage names:
stage_order <- c("Lower Miocene", "Middle Miocene", "Upper Miocene",
  "Plio-Pleistocene")
cross_pred$stage_test <- factor(cross_pred$stage_test,
  levels = stage_order)
cross_pred$stage_train <- factor(cross_pred$stage_train,
  levels = stage_order)

ggplot(cross_pred, aes(pred_self, pred, colour = class)) +
  geom_point(alpha = 0.3) +
  facet_grid(stage_train ~ stage_test) +
  scale_y_log10() + scale_x_log10() +
  scale_colour_brewer(palette = "Set2") +
  xlab("Probability of extinction predicted within the same stage") +
  ylab("Probability of extinction predicted from a different stage") +
  guides(colour = guide_legend(override.aes = list(alpha = 1)))
@

\section{Partial dependence plots}

We will no look at the marginal effect of each predictor on extinction risk. We can do this through partial dependence plots, as shown in Figure 1. This procedure finds the marginal effect of each predictor by integrating across all other predictor values using the method described in J.H. Friedman (2001). Greedy Function Approximation: A Gradient Boosting Machine, Annals of Statistics 29(4). TODO add REF.

Because there's some stochasticity when running a GBM, we will run our models a number of times, and each time record the marginal effects. We will then take the median and interquartile range of those predictions.

First, a function to fit the GBM models to a given stage:

<<shape-data-partial-plots, cache=FALSE>>=
# a function to fit a GBM model to a given stage:
fit_stage_models <- function(dat, stage_name) {
  if(stage_name == "all") {
    d <- dat
  } else {
    d <- dat[dat$stage == stage_name, ]
  }
  w <- paleorisk::get_weights(d$Ex)
  weights <- ifelse(d$Ex == 1, w$ex_weight, w$sur_weight)
  gbm::gbm(Ex ~ richness + occupancy + occurrences + min.lat + max.lat +
    lat.range + mean.lat + great.circle + class + group, data =
    d, n.trees = 300, interaction.depth = 1,
    distribution = "bernoulli", shrinkage = 0.1, weights = weights)
}
@

We will fit the models for each stage. Within each stage we will fit the model a number of times:

<<fit-marginal-stage-models>>=
stage_models <- llply(c(stage_order, "all"), function(x)
  rlply(20, fit_stage_models(stcen2, stage_name = x)))
names(stage_models) <- c(stage_order, "all")
@

And, we will go through the stage-based models and pull out the partial dependence data:

<<pull-out-marginal-data>>=
vars <- c("richness", "occupancy", "occurrences", "min.lat",
  "max.lat", "lat.range", "mean.lat", "great.circle", "group")
partial_dat <- ldply(c(stage_order, "all"), function(stage_name) {
  p2 <- ldply(vars, function(var_name){
    p1 <- ldply(stage_models[[stage_name]], function(x)
      plot.gbm(x, return.grid = TRUE, i.var = var_name,
        type = "response"))
    names(p1) <- c("value", "response")
    p1$predictor <- var_name
    p1
    })
  p2$stage <- stage_name
  p2
  })
@

We will get the median and interquartile range of the marginal effects.

<<summarize-marginal-effects>>=
partial_dat2 <- tbl_df(partial_dat)
partial_dat2 <- group_by(partial_dat2, stage, predictor, value)
partial_dat2 <- summarise(partial_dat2,
  median = median(response),
  lower = quantile(response, probs = 0.25),
  upper = quantile(response, probs = 0.75),
  .n = n())
@

Because the taxonomic variables are factors (i.e.\ they are words) and the other variables are continuous and numeric, we will split these into separate data frames for plotting.

<<separate-taxonomic-and-numeric-predictions>>=
partial_groups <- partial_dat2[partial_dat2$predictor == "group", ]
partial_continuous <- partial_dat2[partial_dat2$predictor != "group", ]
groups_match <- data.frame(value =
  1:length(unique(stcen2$group)),
  group = sort(unique(stcen2$group)), stringsAsFactors = FALSE)
partial_groups <- plyr::join(partial_groups, groups_match)
partial_groups$value <- NULL
partial_groups <- plyr::rename(partial_groups, c("group" = "value"))
@

And we will make some quick plots of the data.

<<plot-partial-continuous, fig.width=10, fig.height=6>>=
ggplot(partial_continuous, aes(value, median)) +
  geom_ribbon(aes(fill = stage, colour = stage, ymax = upper, ymin = lower),
    alpha = 0.5) +
  facet_wrap(~predictor, scales = "free_x") +
  xlab("Predictor value") +
  ylab("Marginal effect on probability of extinction")
@

<<plot-partial-group, out.width="4in">>=
ggplot(subset(partial_groups, median < 0.95), aes(value, median)) +
  geom_pointrange(aes(colour = stage, ymin = lower, ymax = upper)) +
  xlab("Marginal effect on probability of extinction") +
  ylab("") +
  coord_flip()
@

We can re-create Fig.~1 in the paper, which is a more customized version of the previous plots, by sourcing the following file.

TODO fix this Fig 1 plot. TODO add median and interquartile ribbon to these plots. The black lines aren't showing the overall model. Maybe log transform the y axis.

<<partial-plots-base>>=
source("make-partial-dependence-baseplot.R")
@

\section{Merging risk predictions with contemporary species distributions}

We will use our GBM model to predict on the contemporary dataset. We will run our model a number of times and take the average predictions. This is because there is some stochasticity to the GBM algorithm.

<<build-main-paleo-model>>=
w <- paleorisk::get_weights(stcen2$Ex)
weights <- ifelse(stcen2$Ex == 1, w$ex_weight, w$sur_weight)

m <- rlply(20, gbm(Ex ~ richness + occupancy + occurrences + min.lat +
    max.lat + lat.range + mean.lat + great.circle + class + group,
  data = stcen2, n.trees = 300, interaction.depth = 1,
  distribution = "bernoulli", shrinkage = 0.1, weights = weights))
@

Next, we will load the contemporary data.

<<load-modern-data>>=
temp_dat <- readRDS("../data/standardized-predictors-cenozoic-obis.rds")
modern <- subset(temp_dat,
  !class %in% c("Testudines", "Malacostraca") & stage == "Modern")
rm(temp_dat)
@

And predict the contemporary intrinsic extinction risk from the paleontological models. We will take the average prediction across our multiple runs.

<<predict-risk-modern>>=
predictions <- laply(m, function(x)
  gbm::predict.gbm(x, newdata = modern, type = "response",
    n.trees = 300))
modern$pred <- apply(predictions, 2, mean)
@

We will use our calibration factors on our predictions.

<<calibrate-risk-modern>>=
modern$gbm_pred_binned <- assign_bins(modern$pred)
modern <- plyr::join(modern,
  val_test_summarized[,c("class", "gbm_pred_binned", "mean_observed")],
  by = c("class", "gbm_pred_binned"))
# modern <- modern[!is.na(modern$mean_observed), ]
# or skip calibration:
# modern$mean_observed <- modern$pred
@

We will take a look at calibrated predictions vs.\ the raw model predictions (with weighted observations):

<<plot-calibrations, fig.height=4>>=
ggplot(modern, aes(pred, mean_observed)) + geom_point() +
  facet_wrap(~class) + coord_fixed(ratio = 1) +
  xlim(0, 0.9) + ylim(0, 0.9) +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  xlab("Predicted") + ylab("Calibrated prediction")
@

We will also save a version of our predictions where we build the predictions based on the average predictions across geologic stage models. We've already built these models. We will take the average predictions across replicated fits and geologic stages.

<<predict-risk-modern-average-version>>=
predictions_stage_avg <- laply(stage_models, function(x) {
    p1 <- laply(x, function(y) gbm::predict.gbm(y, newdata = modern,
  type = "response", n.trees = 300))
  p2 <- apply(p1, 2, mean)
  p2
})
modern$pred_stage_avg <- apply(predictions_stage_avg, 2, mean)
@

We will save the \texttt{modern} data frame for future use.

<<save-modern>>=
saveRDS(modern, file = "../data/modern-predictions.rds")
@

We can compare the predictions from the averaged stage models with the predictions from the GBMs based on all Neogene data pooled together. Note that we're not comparing the calibrated predictions here:

<<compare-neogene-gbm-to-stage-models, fig.height=4.5>>=
ggplot(modern, aes(pred, pred_stage_avg)) + geom_point() +
  facet_wrap(~class) + geom_abline(intercept = 0, slope = 1) +
  xlab("Neoegene GBM") + ylab("Stage-level GBMs averaged")

# TODO TEMP:
# modern$mean_observed <- modern$pred_stage_avg
@

Next, we will merge these predictions with our contemporary spatial observations. At the same time, we will merge in Halpern et al. and Spalding et al.

TODO Note which files this creates.

<<merge-predictions-spatial, cache=TRUE>>=
# read in the modern genus-occupancy data:
d.eco.filled <- readRDS("../data/modern-province-occupancy.rds")
Input_ranges <- "Interpolated"
d.eco.filled <- gdata::drop.levels(subset(d.eco.filled, d.eco.filled$Ranges ==
    Input_ranges))
d.eco.filled$Ranges <- NULL

# bring in the modern predictions:
modern <- readRDS("../data/modern-predictions.rds")

# merge occurrence data with risk predictions:
d.eco.filled <- merge(d.eco.filled, modern, by = "genus", all.x = FALSE)

# and calculate mean values by class-province or for the provinces overall:
by.prov.classes <- summarise(group_by(d.eco.filled, class, PROV_CODE), mean.ext
  = mean(log(mean_observed + 0.001)), median.ext = median(mean_observed), N.gen
  = length(unique(genus)), mean.occupancy = mean(occupancy), mean.occurrences =
  mean(occurrences), mean.min.lat = mean(min.lat), mean.max.lat =
  mean(max.lat), mean.mean.lat = mean(mean.lat), mean.gcd = mean(great.circle),
  mean.lat.range = mean(lat.range), mean.richness = mean(richness))

by.prov.all <- summarise(group_by(d.eco.filled, PROV_CODE), mean.ext =
  mean(log(mean_observed + 0.001)), median.ext = median(mean_observed), N.gen =
  length(unique(genus)), mean.occupancy = mean(occupancy), mean.occurrences =
  mean(occurrences), mean.min.lat = mean(min.lat), mean.max.lat =
  mean(max.lat), mean.mean.lat = mean(mean.lat), mean.gcd = mean(great.circle),
  mean.lat.range = mean(lat.range), mean.richness = mean(richness))

# A second version that takes the mean of the class means:
by.prov.all2 <- summarise(group_by(by.prov.classes, PROV_CODE),
  mean.ext = mean(mean.ext))

# read in province area measurements and OBIS sampling data:
Prov.Areas <- read.csv("../data/spalding-province-areas.csv", header = TRUE,
  stringsAsFactors = FALSE)
Prov.Sampling <- read.csv("../data/obis.sampling.and.richness.csv", header =
  TRUE, stringsAsFactors = FALSE)
Prov.Data <- merge(Prov.Sampling, Prov.Areas, by = c("Zone", "PROV_CODE"),
  all.x = TRUE)
by.prov.classes <- merge(by.prov.classes, Prov.Data, by = c("class", "PROV_CODE"))
by.prov.all <- merge(by.prov.all, Prov.Data, by = "PROV_CODE")

# read in Halpern et al. and Burrows et al. layers
Impacts <- read.csv("../data/Halpern_Burrows_by_Spalding.csv", header = TRUE)
Impacts <- data.frame(Impacts$PROV_CODE, scale(Impacts$Mean_Halpern_Province),
  scale(Impacts$Mean_Burrows_Province))
colnames(Impacts) <- c("PROV_CODE", "Halpern", "Burrows")
by.prov.all <- merge(by.prov.all, Impacts, by = "PROV_CODE")
by.prov.all <- gdata::drop.levels(subset(by.prov.all, by.prov.all$class == "all"))
by.prov.classes <- merge(by.prov.classes, Impacts, by = "PROV_CODE")
by.prov.all$log_OBIS_records <- log(by.prov.all$OBIS_records)
by.prov.classes$log_OBIS_records <- log(by.prov.classes$OBIS_records)

by.prov.classes$Lat_Zone <- as.factor(ifelse(by.prov.classes$Zone ==
    "Tropical", "Tropical", "Extratropical"))
by.prov.all$Lat_Zone <- as.factor(ifelse(by.prov.all$Zone == "Tropical",
    "Tropical", "Extratropical"))

save(by.prov.classes, file = "../data/by.prov.classes.rda")
save(by.prov.all, file = "../data/by.prov.all.rda")
@

We can look at the distribution of predicted extinction probability. Note that the x-axis is log distributed:

<<plot-density-risk, fig.height=4>>=
ggplot(d.eco.filled, aes(mean_observed + 0.001)) +
  geom_histogram(binwidth = 0.3) + facet_wrap(~class) + scale_x_log10() +
  ylab("Density") + xlab("Predicted extinction probabilty")
@

We can also look at the distribution of our predictions by province and class. This code produces Figure~\ref{fig:plot-density-risk-by-province}.

<<plot-density-risk-by-province, fig.width=17, fig.height=13, out.width="7in", fig.cap="The distribution of intrinsic extinction risk by ecoprovince (panels) and taxonomic class (colours). Note the log transformed x-axis.">>=
# merge in province names:
er <- readShapePoly("../data/MEOW2/meow_ecos.shp")
prov_names <- er@data[,c("PROVINCE", "PROV_CODE")]
prov_names <- prov_names[!duplicated(prov_names), ]
temp <- plyr::join(d.eco.filled, prov_names)
temp <- transform(temp, PROV_pretty = paste(PROV_CODE, PROVINCE))
ggplot(temp, aes(mean_observed + 0.001)) +
  geom_histogram(aes(fill = class, colour = class),
    position = "stack", alpha = 0.5, binwidth = 0.4) +
  facet_wrap(~ PROV_pretty) +
  scale_x_log10() +
  ylab("Number of genera") + xlab("Predicted extinction probabilty")
@

\section{The role of the tropics}

We can inspect the role of tropical regions. Earlier we defined ``tropical'' as less than 30$^{\circ}$ absolute latitude. This code produces Figure~\ref{fig:test-tropical-drivers}.

<<test-tropical-drivers, fig.width=18, fig.height=11.5, out.width="7in", fig.cap="log(Predicted extinction probability) vs.\\ various predictors across taxonomic classes. Tropical regions are shown with blue lines and extratropical regions (everything else) are shown with organge lines.">>=
tropical_test <- reshape2::melt(by.prov.classes, id.vars =
  c("PROV_CODE", "class", "Lat_Zone"), measure.vars =
  c("mean.lat.range", "mean.occupancy", "mean.gcd", "log_OBIS_records",
    "mean.richness"))
tropical_test <- plyr::join(tropical_test, by.prov.classes[, c("PROV_CODE",
    "class", "Lat_Zone", "mean.ext")])

ggplot(tropical_test,
  aes(value, mean.ext, colour = Lat_Zone, shape = Lat_Zone)) +
  geom_point(alpha = 0.7) +
  facet_wrap(variable~class, scales = "free") +
  stat_smooth(method = "lm", se = FALSE, lwd = 1.5)
@

\section{Mapping extinction probabilities}

<<plot-class-maps, cache=FALSE>>=
# source("plot-class-ext-maps.R")
@

<<plot-hotspots, cache=FALSE>>=
# create the dataset to map:
load("../data/by.prov.all.rda")
er <- maptools::readShapePoly("../data/MEOW2/meow_ecos.shp")
er@data$id = rownames(er@data)
er_points = ggplot2::fortify(er, region = "id")
er_dat <- plyr::join(er_points, er@data, by = "id")
er_dat <- plyr::join(er_dat, by.prov.all, by = "PROV_CODE")

pdf("../figs/hotspots.pdf", width = 7, height = 4)
map_hotspots(er_dat)
dev.off()
@

Let's look at the effect of the minimum-genera-per-province cutoff:

<<min-gen-thresh-sensitivity, fig.height=8, fig.width=13>>=
par(mfrow = c(2, 2))
par(mar = c(0, 0, 2, 0), oma = c(2, 2, 2, 2))
map_hotspots(er_dat, min_prov_genera = 25, hotspots = FALSE)
mtext("Min. genera = 25", cex = 2)
map_hotspots(er_dat, min_prov_genera = 50, hotspots = FALSE)
mtext("Min. genera = 50", cex = 2)
map_hotspots(er_dat, min_prov_genera = 100, hotspots = FALSE)
mtext("Min. genera = 100", cex = 2)
map_hotspots(er_dat, min_prov_genera = 200, hotspots = FALSE)
mtext("Min. genera = 200", cex = 2)
@


\clearpage

Things to add to the modelling supp discussion:

\begin{itemize}
  \item standardizing of data by interval first --- what and why
  \item why a machine learning approach in the first place (minimal assumptions, non-linear relationships, maximal predictive performance)
  \item what a GBM is
  \item there are many bias-reduction benefits to GBMs over random forests
  \item tuning parameters for a GBM, how we tuned
  \item why we needed to test and calibrate at the class level --- model built across all classes to pool habitat information, unlike multi-level parametric modelling there is no straightforward way to partially pool info in a GBM, therefore we tested and calibrated at the class level post-modelling \ldots important because we are generating maps at the class level, and so we want to minimize bias at the class level while modelling at aggregate level\ldots plus even without the class-level issue, out-of-sample performance can be non-linearly biased
  \item what a partial dependence plot shows (Fig 1)
  \item the cross-interval prediction test: reason for doing it and the plot\ldots no calibration here
\end{itemize}

Supp Figures to merge in:

\begin{enumerate}
  \item density plots of the distributions of intrinsic risk in different phylogenetic groups for each of the 62 provinces
  \item number of OBIS occurrence records in class-province maps (use different colour scale to before)
  \item re-create class-level maps omit single-province genera (actually, do this for main fig now)?
  \item re-create class-level maps but base on raw OBIS ranges
  \item re-create class-level maps but with raw (not standardized) predictors
  \item re-create class-level maps but with rank-order risk
  \item partial rank dependence vs.\ mean Cenozoic completeness
  \item the cross plots of human threat, climate velocity, and intrinsic risk on global map (from what was a panel in the last main Fig)
\end{enumerate}

\clearpage
\bibliographystyle{science}
\bibliography{jshort,risksupp}

\clearpage

Main-text figures (here temporarily for reference).

\begin{center}
  \includegraphics[width=4.86in]{../figs/partial-plot-base.pdf}\\
  \smallskip
  Figure 1
\end{center}

\clearpage

\begin{center}
  \includegraphics[width=4.86in]{../figs/class-risk-maps-mean-log-ext.pdf}\\
  \smallskip
  Figure 2
\end{center}

\clearpage

\begin{center}
  \includegraphics[width=4.86in]{../figs/hotspots.pdf}\\
  \smallskip
  Figure 3
\end{center}

\newpage

\section{Supplementary Figures}

\end{document}
